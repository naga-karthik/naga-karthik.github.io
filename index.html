<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Enamundram Naga Karthik</title> <meta name="author" content="Enamundram Naga Karthik"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://naga-karthik.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About Me<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">GitHub</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Enamundram</span> Naga Karthik </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p>Hi there! I‚Äôm Naga, a fourth-year PhD student at <a href="https://neuro.polymtl.ca" rel="external nofollow noopener" target="_blank">NeuroPoly</a>, Polytechnique Montr√©al. I‚Äôm also affiliated to <a href="https://mila.quebec/en" rel="external nofollow noopener" target="_blank">Mila - Qu√©bec AI Institute</a>. My research focuses on developing deep learning-based methods for medical image analysis, with a particular interest in spinal cord imaging using real-world clinical data. My current projects include contrast-agnostic segmentation of the spinal cord and the segmentation of lesions in spinal cord injury and multiple sclerosis (MS). I have also experimented with generative modelling, in particular, training GANs for cross-modality MR-CT synthesis and diffusion models for synthesizing spinal cord lesions. In the past, I have also worked on the application of continual/lifelong learning methods for the segmentation of brain MS lesions. Please head to my <a href="https://scholar.google.com/citations?user=ZryIoMMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Google Scholar</a> profile for a complete list of my publications.</p> <p>When I am not working, you‚Äôll find me either running <img class="emoji" title=":running:" alt=":running:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c3.png" height="20" width="20">, hiking <img class="emoji" title=":mountain:" alt=":mountain:" src="https://github.githubassets.com/images/icons/emoji/unicode/26f0.png" height="20" width="20">, or reading <img class="emoji" title=":book:" alt=":book:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png" height="20" width="20">.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Mar 2025</th> <td> I was featured in the <a href="https://www.unique.quebec" rel="external nofollow noopener" target="_blank">UNIQUE-Qu√©bec</a> Newsletter as part of their student spotlight series! Please find the newsletter <a href="https://drive.google.com/file/d/1QAUg3TwfB2ygj8cFpwZACkK-9L18Z75C/view?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>. üì∞ </td> </tr> <tr> <th scope="row">Jan 2025</th> <td> Our paper ‚Äú<a href="https://www.sciencedirect.com/science/article/pii/S1361841525000210" rel="external nofollow noopener" target="_blank">Towards contrast-agnostic soft segmentation of the spinal cord</a>‚Äù was accepted at the Medical Image Analysis journal (Impact Factor: 10.7)! üéâ </td> </tr> <tr> <th scope="row">Jul 2024</th> <td> Our paper ‚Äú<a href="https://arxiv.org/pdf/2407.17265" rel="external nofollow noopener" target="_blank">SCIsegV2: A Universal Tool for Segmentation of Intramedullary Lesions in Spinal Cord Injury</a>‚Äù was accepted at MICCAI Applications of Medical Artificial Intelligence (AMAI) Workshop 2024 in Marrakech, Morocco! üá≤üá¶ </td> </tr> <tr> <th scope="row">Apr 2024</th> <td> Our spin-off project <a href="https://openreview.net/pdf?id=n6D25aqdV3" rel="external nofollow noopener" target="_blank">comparing different DL architectures for contrast-agnostic spinal cord segmentation</a> was accepted at the MIDL 2024 Short Paper Track! </td> </tr> <tr> <th scope="row">Mar 2024</th> <td> I was awarded the <a href="https://www.daad-canada.ca/en/find-funding/graduate-opportunities/research-grants/short-term-research-grants/" rel="external nofollow noopener" target="_blank">DAAD Short-term Research Grant</a> for a 4-month research stay at the Technical University of Munich, Germany! ü•® üá©üá™ </td> </tr> <tr> <th scope="row">Feb 2024</th> <td> Our recent works on contrast-agnostic spinal cord segmentation and segmentation of spinal cord injury lesions were accepted for Oral Presentations at the 2024 ISMRM &amp; ISMRT Annual Meeting &amp; Exhibition in Singapore <img class="emoji" title=":singapore:" alt=":singapore:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f8-1f1ec.png" height="20" width="20"> ! </td> </tr> <tr> <th scope="row">Nov 2023</th> <td> I gave a talk on Automatic Segmentation of Brain and Spinal Cord Lesions across Pathologies at the <a href="https://unique-students.github.io" rel="external nofollow noopener" target="_blank">UNIQUE Fellows Get-Together</a>, held at <a href="https://mila.quebec/en" rel="external nofollow noopener" target="_blank">Mila</a>! </td> </tr> <tr> <th scope="row">Aug 2023</th> <td> I gave a talk on Continual Learning for Medical Image Segmentation at the <a href="https://chandar-lab.github.io/CRLSymposium/2023" rel="external nofollow noopener" target="_blank">Chandar Lab Symposium</a>, held at <a href="https://mila.quebec/en" rel="external nofollow noopener" target="_blank">Mila</a>! </td> </tr> <tr> <th scope="row">Jun 2022</th> <td> Honoured to receive the <a href="https://www.gg.ca/en/honours/governor-generals-awards/governor-generals-academic-medal" rel="external nofollow noopener" target="_blank">Governor General of Canada‚Äôs Academic Gold Medal</a> for outstanding academic achievements during my master‚Äôs at √âTS! üèÖ üá®üá¶ </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"><abbr class="badge" style="background-color:#003DA5"><a href="https://www.medrxiv.org/" rel="external nofollow noopener" target="_blank">medRxiv</a></abbr></div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/msseg_bavaria-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/msseg_bavaria-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/msseg_bavaria-1400.webp"></source> <img src="/assets/img/publication_preview/msseg_bavaria.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="msseg_bavaria.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div id="NagaKarthik2025.01.22.25320959" class="col-sm-8"> <div class="title">Automatic segmentation of spinal cord lesions in MS: A robust tool for axial T2-weighted MRI scans</div> <div class="author"> <em>Enamundram Naga Karthik*</em>,¬†Julian McGinnis*,¬†Ricarda Wurm,¬†Sebastian Ruehling,¬†Robert Graf,¬†Jan Valosek,¬†Pierre-Louis Benveniste,¬†Markus Lauerer,¬†Jason Talbott,¬†Rohit Bakshi,¬†Shahamat Tauhid,¬†Timothy Shepherd,¬†Achim Berthele,¬†Claus Zimmer,¬†Bernhard Hemmer,¬†Daniel Rueckert,¬†Benedikt Wiestler,¬†Jan S. Kirschke,¬†Julien Cohen-Adad,¬†and¬†Mark M√ºhlau</div> <div class="periodical"> <em>medRxiv</em>, 2025 </div> <div class="periodical"> *shared first authorship </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.medrxiv.org/content/early/2025/01/23/2025.01.22.25320959.full.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ivadomed/model-seg-ms-axial-t2w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1101/2025.01.22.25320959"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1101/2025.01.22.25320959" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Deep learning models have achieved remarkable success in segmenting brain white matter lesions in multiple sclerosis (MS), becoming integral to both research and clinical workflows. While brain lesions have gained significant attention in MS research, the involvement of spinal cord lesions in MS is relatively understudied. This is largely owed to the variability in spinal cord magnetic resonance imaging (MRI) acquisition protocols, high individual anatomical differences, the complex morphology and size of spinal cord lesions - and lastly, the scarcity of labeled datasets required to develop robust segmentation tools. As a result, automatic segmentation of spinal cord MS lesions remains a significant challenge. Although some segmentation tools exist for spinal cord lesions, most have been developed using sagittal T2-weighted (T2w) sequences primarily focusing on cervical spines. With the growing importance of spinal cord imaging in MS, axial T2w scans are becoming increasingly relevant due to their superior sensitivity in detecting lesions compared to sagittal acquisition protocols. However, most existing segmentation methods struggle to effectively generalize to axial sequences due to differences in image characteristics caused by the highly anisotropic spinal cord scans. To address these challenges, we developed a robust, open-source lesion segmentation tool tailored specifically for axial T2w scans covering the whole spinal cord. We investigated key factors influencing lesion segmentation, including the impact of stitching together individually acquired spinal regions, straightening the spinal cord, and comparing the effectiveness of 2D and 3D convolutional neural networks (CNNs). Drawing on these insights, we trained a multi-center model using an extensive dataset of 582 MS patients, resulting in a dataset comprising an entirety of 2,167 scans. We empirically evaluated the model‚Äôs segmentation performance across various spinal segments for lesions with varying sizes. Our model significantly outperforms the current state-of-the-art methods, providing consistent segmentation across cervical, thoracic and lumbar regions. To support the broader research community, we integrate our model into the widely-used Spinal Cord Toolbox (v7.0 and above).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">NagaKarthik2025.01.22.25320959</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Naga Karthik*, Enamundram and McGinnis*, Julian and Wurm, Ricarda and Ruehling, Sebastian and Graf, Robert and Valosek, Jan and Benveniste, Pierre-Louis and Lauerer, Markus and Talbott, Jason and Bakshi, Rohit and Tauhid, Shahamat and Shepherd, Timothy and Berthele, Achim and Zimmer, Claus and Hemmer, Bernhard and Rueckert, Daniel and Wiestler, Benedikt and Kirschke, Jan S. and Cohen-Adad, Julien and M{\"u}hlau, Mark}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic segmentation of spinal cord lesions in MS: A robust tool for axial T2-weighted MRI scans}</span><span class="p">,</span>
  <span class="na">elocation-id</span> <span class="p">=</span> <span class="s">{2025.01.22.25320959}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1101/2025.01.22.25320959}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory Press}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.medrxiv.org/content/early/2025/01/23/2025.01.22.25320959}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{medRxiv}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{*shared first authorship}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"><abbr class="badge" style="background-color:#004d00"><a href="https://www.sciencedirect.com/journal/medical-image-analysis" rel="external nofollow noopener" target="_blank">MedIA</a></abbr></div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/contrast_agnostic_v2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/contrast_agnostic_v2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/contrast_agnostic_v2-1400.webp"></source> <img src="/assets/img/publication_preview/contrast_agnostic_v2.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="contrast_agnostic_v2.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div id="BEDARD2025103473" class="col-sm-8"> <div class="title">Towards contrast-agnostic soft segmentation of the spinal cord</div> <div class="author"> Sandrine B√©dard*,¬†<em>Enamundram Naga Karthik*</em>,¬†Charidimos Tsagkas,¬†Emanuele Pravat√†,¬†Cristina Granziera,¬†Andrew Smith,¬†Kenneth Arnold Weber II,¬†and¬†Julien Cohen-Adad</div> <div class="periodical"> <em>Medical Image Analysis</em>, 2025 </div> <div class="periodical"> *shared first authorship </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1361841525000210" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/sct-pipeline/contrast-agnostic-softseg-spinalcord" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1016/j.media.2025.103473"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1016/j.media.2025.103473" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Spinal cord segmentation is clinically relevant and is notably used to compute spinal cord cross-sectional area (CSA) for the diagnosis and monitoring of cord compression or neurodegenerative diseases such as multiple sclerosis. While several semi and automatic methods exist, one key limitation remains: the segmentation depends on the MRI contrast, resulting in different CSA across contrasts. This is partly due to the varying appearance of the boundary between the spinal cord and the cerebrospinal fluid that depends on the sequence and acquisition parameters. This contrast-sensitive CSA adds variability in multi-center studies where protocols can vary, reducing the sensitivity to detect subtle atrophies. Moreover, existing methods enhance the CSA variability by training one model per contrast, while also producing binary masks that do not account for partial volume effects. In this work, we present a deep learning-based method that produces soft segmentations of the spinal cord that are stable across MRI contrasts. Using the Spine Generic Public Database of healthy participants (n=267; contrasts=6), we first generated participant-wise soft ground truth (GT) by averaging the binary segmentations across all 6 contrasts. These soft GT, along with aggressive data augmentation and a regression-based loss function, were then used to train a U-Net model for spinal cord segmentation. We evaluated our model against state-of-the-art methods and performed ablation studies involving different GT mask types, loss functions, contrast-specific models and domain generalization methods. Our results show that using the soft average segmentations along with a regression loss function reduces CSA variability (p&lt;0.05, Wilcoxon signed-rank test). The proposed spinal cord segmentation model generalizes better than the state-of-the-art contrast-specific methods amongst unseen datasets, vendors, contrasts, and pathologies (compression, lesions), while accounting for partial volume effects. Our model is integrated into the Spinal Cord Toolbox (v6.2 and higher).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">BEDARD2025103473</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards contrast-agnostic soft segmentation of the spinal cord}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Medical Image Analysis}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{101}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103473}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1361-8415}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.media.2025.103473}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S1361841525000210}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{B√©dard*, Sandrine and Naga Karthik*, Enamundram and Tsagkas, Charidimos and Pravat√†, Emanuele and Granziera, Cristina and Smith, Andrew and {Weber II}, Kenneth Arnold and Cohen-Adad, Julien}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Spinal cord, MRI, Contrasts, Segmentation, Deep learning, Soft labels, Partial volume effect}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{*shared first authorship}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"><abbr class="badge" style="background-color:#ab7343"><a href="https://pubs.rsna.org/journal/ai" rel="external nofollow noopener" target="_blank">Radiology: AI</a></abbr></div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sciseg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sciseg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sciseg-1400.webp"></source> <img src="/assets/img/publication_preview/sciseg.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sciseg.jpeg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div id="doi:10.1148/ryai.240005" class="col-sm-8"> <div class="title">SCIseg: Automatic Segmentation of Intramedullary Lesions in Spinal Cord Injury on T2-weighted MRI Scans</div> <div class="author"> <em>Enamundram Naga Karthik*</em>,¬†Jan Valo≈°ek*,¬†Andrew C. Smith,¬†Dario Pfyffer,¬†Simon Schading-Sassenhausen,¬†Lynn Farner,¬†Kenneth A. Weber,¬†Patrick Freund,¬†and¬†Julien Cohen-Adad</div> <div class="periodical"> <em>Radiology: Artificial Intelligence</em>, 2025 </div> <div class="periodical"> *shared first authorship </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1148/ryai.240005" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ivadomed/model_seg_sci/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Purpose: To develop a deep learning tool for the automatic segmentation of the spinal cord and intramedullary lesions in spinal cord injury (SCI) on T2-weighted MRI scans. Materials and Methods: This retrospective study included MRI data acquired between July 2002 and February 2023. The data consisted of T2-weighted MRI scans acquired using different scanner manufacturers with various image resolutions (isotropic and anisotropic) and orientations (axial and sagittal). Patients had different lesion etiologies (traumatic, ischemic, and hemorrhagic) and lesion locations across the cervical, thoracic, and lumbar spine. A deep learning model, SCIseg (which is open source and accessible through the Spinal Cord Toolbox, version 6.2 and above), was trained in a three-phase process involving active learning for the automatic segmentation of intramedullary SCI lesions and the spinal cord. The segmentations from the proposed model were visually and quantitatively compared with those from three other open-source methods (PropSeg, DeepSeg, and contrast-agnostic, all part of the Spinal Cord Toolbox). The Wilcoxon signed rank test was used to compare quantitative MRI biomarkers of SCI (lesion volume, lesion length, and maximal axial damage ratio) derived from the manual reference standard lesion masks and biomarkers obtained automatically with SCIseg segmentations. Results: The study included 191 patients with SCI (mean age, 48.1 years ¬± 17.9 [SD]; 142 [74%] male patients). SCIseg achieved a mean Dice score of 0.92 ¬± 0.07 and 0.61 ¬± 0.27 for spinal cord and SCI lesion segmentation, respectively. There was no evidence of a difference between lesion length (P = .42) and maximal axial damage ratio (P = .16) computed from manually annotated lesions and the lesion segmentations obtained using SCIseg. Conclusion: SCIseg accurately segmented intramedullary lesions on a diverse dataset of T2-weighted MRI scans and automatically extracted clinically relevant lesion characteristics. Keywords: Spinal Cord, Trauma, Segmentation, MR Imaging, Supervised Learning, Convolutional Neural Network (CNN) Published under a CC BY 4.0 license. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">doi:10.1148/ryai.240005</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Naga Karthik*, Enamundram and Valo\v{s}ek*, Jan and Smith, Andrew C. and Pfyffer, Dario and Schading-Sassenhausen, Simon and Farner, Lynn and Weber, Kenneth A. and Freund, Patrick and Cohen-Adad, Julien}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SCIseg: Automatic Segmentation of Intramedullary Lesions in Spinal Cord Injury on T2-weighted MRI Scans}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Radiology: Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e240005}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{*shared first authorship}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"><abbr class="badge">IEEE OJEMB</abbr></div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ojemb_uncertainty_scoliosis-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ojemb_uncertainty_scoliosis-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ojemb_uncertainty_scoliosis-1400.webp"></source> <img src="/assets/img/publication_preview/ojemb_uncertainty_scoliosis.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ojemb_uncertainty_scoliosis.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div id="10086579" class="col-sm-8"> <div class="title">Uncertainty Estimation in Unsupervised MR-CT Synthesis of Scoliotic Spines</div> <div class="author"> <em>Enamundram Naga Karthik</em>,¬†Farida Cheriet,¬†and¬†Catherine Laporte</div> <div class="periodical"> <em>IEEE Open Journal of Engineering in Medicine and Biology</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10086579" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/naga-karthik/3D-CycleGAN-with-Uncertainty" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1109/OJEMB.2023.3262965"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1109/OJEMB.2023.3262965" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Uncertainty estimations through approximate Bayesian inference provide interesting insights to deep neural networks‚Äô behavior. In unsupervised learning tasks, where expert labels are unavailable, it becomes ever more important to critique the model through uncertainties. This paper presents a proof-of-concept for generalizing the aleatoric and epistemic uncertainties in unsupervised MR-CT synthesis of scoliotic spines. A novel adaptation of the cycle-consistency constraint in CycleGAN is proposed such that the model predicts the aleatoric uncertainty maps in addition to the standard volume-to-volume translation between Magnetic Resonance (MR) and Computed Tomography (CT) data. Ablation experiments were performed to understand uncertainty estimation as an implicit regularizer and a measure of the model‚Äôs confidence. The aleatoric uncertainty helps in distinguishing between the bone and soft-tissue regions in CT and MR data during translation, while the epistemic uncertainty provides interpretable information to the user for downstream tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10086579</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Naga Karthik, Enamundram and Cheriet, Farida and Laporte, Catherine}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Open Journal of Engineering in Medicine and Biology}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Uncertainty Estimation in Unsupervised MR-CT Synthesis of Scoliotic Spines}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-7}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/OJEMB.2023.3262965}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <div class="abbr"><abbr class="badge">MELBA</abbr></div> <div class="preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/melba_uncertainty_softseg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/melba_uncertainty_softseg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/melba_uncertainty_softseg-1400.webp"></source> <img src="/assets/img/publication_preview/melba_uncertainty_softseg.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="melba_uncertainty_softseg.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div id="melba:2022:031:lemay" class="col-sm-8"> <div class="title">Label fusion and training methods for reliable representation of inter-rater uncertainty</div> <div class="author"> Andreanne Lemay,¬†Charley Gros,¬†<em>Enamundram Naga Karthik</em>,¬†and¬†Julien Cohen-Adad</div> <div class="periodical"> <em>Machine Learning for Biomedical Imaging</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.melba-journal.org/pdf/2022:031.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.59275/j.melba.2022-db5c"></span> <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.59275/j.melba.2022-db5c" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Medical tasks are prone to inter-rater variability due to multiple factors such as image quality, professional experience and training, or guideline clarity. Training deep learning networks with annotations from multiple raters is a common practice that mitigates the model‚Äôs bias towards a single expert. Reliable models generating calibrated outputs and reflecting the inter-rater disagreement are key to the integration of artificial intelligence in clinical practice. Various methods exist to take into account different expert labels. We focus on comparing three label fusion methods: STAPLE, average of the rater‚Äôs segmentation, and random sampling of each rater‚Äôs segmentation during training. Each label fusion method is studied using both the conventional training framework and the recently published SoftSeg framework that limits information loss by treating the segmentation task as a regression. Our results, across 10 data splittings on two public datasets (spinal cord gray matter challenge, and multiple sclerosis brain lesion segmentation), indicate that SoftSeg models, regardless of the ground truth fusion method, had better calibration and preservation of the inter-rater rater variability compared with their conventional counterparts without impacting the segmentation performance. Conventional models, i.e., trained with a Dice loss, with binary inputs, and sigmoid/softmax final activate, were overconfident and underestimated the uncertainty associated with inter-rater variability. Conversely, fusing labels by averaging with the SoftSeg framework led to underconfident outputs and overestimation of the rater disagreement. In terms of segmentation performance, the best label fusion method was different for the two datasets studied, indicating this parameter might be task-dependent. However, SoftSeg had segmentation performance systematically superior or equal to the conventionally trained models and had the best calibration and preservation of the inter-rater variability. SoftSeg has a low computational cost and performed similarly in terms of uncertainty to ensembles which require multiple models and forward passes. Our code is available at https://ivadomed.org.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">melba:2022:031:lemay</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Label fusion and training methods for reliable representation of inter-rater uncertainty}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lemay, Andreanne and Gros, Charley and Naga Karthik, Enamundram and Cohen-Adad, Julien}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Machine Learning for Biomedical Imaging}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{January 2023 issue}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--27}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2766-905X}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.59275/j.melba.2022-db5c}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://melba-journal.org/2022:031}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%65%6D%76%6E%61%67%61%6B%61%72%74%68%69%6B@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-2940-5514" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=ZryIoMMAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/naga-karthik" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/naga-karthik-enamundram-7b1559174" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/naga_karthik7" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> Interested in collaborating or just want to say hi? Drop me an email! </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Enamundram Naga Karthik. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 01, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-2GT79X63MT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-2GT79X63MT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>