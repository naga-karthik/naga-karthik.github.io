<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What Statistical Tests Should I Perform? | Naga Karthik Enamundram </title> <meta name="author" content="Naga Karthik Enamundram"> <meta name="description" content="A guide to choosing the right statistical tests for your next paper."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://naga-karthik.github.io/blog/2025/what-statistical-tests/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Naga Karthik</span> Enamundram </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">What Statistical Tests Should I Perform?</h1> <p class="post-meta"> Created on September 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a>   ·   <a href="/blog/category/resources"> <i class="fa-solid fa-tag fa-sm"></i> resources</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"> <a href="#definitions">Definitions</a> <ul> <li class="toc-entry toc-h4"><a href="#outcome-dependent-variable-types">Outcome (dependent) variable types</a></li> <li class="toc-entry toc-h4"><a href="#exposure-independent-variable-types">Exposure (independent) variable types</a></li> <li class="toc-entry toc-h4"><a href="#errors-in-hypothesis-testing">Errors in hypothesis testing</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#choosing-the-right-test">Choosing the right test</a> <ul> <li class="toc-entry toc-h4"><a href="#parametric-or-non-parametric">Parametric or Non-parametric?</a></li> <li class="toc-entry toc-h4"><a href="#post-hoc-tests">Post-hoc tests</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#examples">Examples</a> <ul> <li class="toc-entry toc-h4"><a href="#example-1-comparing-accuracy-between-five-models-on-a-fixed-test-set">Example 1: Comparing accuracy between five models on a fixed test set</a></li> <li class="toc-entry toc-h4"><a href="#example-2-comparing-ages-of-patients-in-train-and-test-sets">Example 2: Comparing ages of patients in train and test sets</a></li> <li class="toc-entry toc-h4"><a href="#example-3-comparing-model-predictions-categorical-between-three-models-on-a-fixed-test-set">Example 3: Comparing model predictions (categorical) between three models on a fixed test set</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#general-notes-and-common-pitfalls">General Notes and Common Pitfalls</a> <ul> <li class="toc-entry toc-h4"><a href="#choosing-significance-level-alpha">Choosing significance level (alpha)</a></li> <li class="toc-entry toc-h4"><a href="#ignoring-the-multiple-comparisons-problem">Ignoring the multiple comparisons problem</a></li> <li class="toc-entry toc-h4"><a href="#when-to-not-rely-purely-on-p-values">When to not rely purely on p-values</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>Too often, I see tables in ML papers where authors claim that their method is “significantly” better than the rest of the methods without performing any statistical tests. I feel that such claims should be backed up by statistical evidence and not just concluding from the magnitude of the metrics reported. Even if the error intervals are reported, the top methods tend to lie with the standard deviation of each other, making it hard to conclude which method is <em>actually</em> better. Therefore, in this post, I intend to cover a few recurring scenarios when presenting results in ML papers and what kind of tests could be performed. Choosing the right test is tricky but it is worth the effort as results backed by statistical evidence seem more credible. This post is inspired from my own struggles in deciding what tests to do and how I wished there was a guide for non-stats folks to refer to when it’s time to put results into the paper.</p> <p>Here’s a flowchart on the most common types of tests depending on the type of the output variable followed by the series of tests to consider.</p> <d-figure> <img src="/assets/img/blog_pictures/stats/stats_tests_flowchart.svg" alt="A flowchart for choosing the right statistical test." class="img-fluid rounded"> <figcaption style="text-align: center;"> A flowchart for choosing the right statistical test. Adapted from <a href="https://www.med.soton.ac.uk/resmethods/statisticalnotes/which_test_flow.htm" target="_blank" rel="external nofollow noopener">this source</a>. </figcaption> </d-figure> <h3 id="definitions">Definitions</h3> <h4 id="outcome-dependent-variable-types">Outcome (dependent) variable types</h4> <ul> <li> <strong>Continuous variables</strong>: Think measuring tape. Continuous variables represent values that can be measured on a continuous scale. Their values are not restricted to separate, distinct numbers. For example: <ul> <li>Performance metrics such as the F1-score, MSE; values whose range lies between 0-1.</li> <li>Features such as a person’s height, average temperature, etc.</li> </ul> </li> <li> <strong>Categorical variables</strong>: Think groups or labeled buckets. Categorical variables only take on a limited set of values, assigning each data point to a particular group. There are two types of variables: (i) <strong>Nominal</strong>: categories have <em>no</em> intrinsic order (e.g. car, bus, plane), and (ii) <strong>Ordinal</strong>: categories have a meaniningful order (e.g. rating something as <code class="language-plaintext highlighter-rouge">good</code>, <code class="language-plaintext highlighter-rouge">okay</code>, <code class="language-plaintext highlighter-rouge">bad</code>, or, levels of difficulty, etc.). A few examples of categorial variables: <ul> <li>Model prediction as <code class="language-plaintext highlighter-rouge">Spam</code>/<code class="language-plaintext highlighter-rouge">Not spam</code>, or, classification of a bacterium as <code class="language-plaintext highlighter-rouge">Resistant</code>/<code class="language-plaintext highlighter-rouge">Susceptible</code> against a specific antibiotic.</li> <li>Features such as grading of a tumor on a scale of 1-4 (note: this is ordinal), or, a person’s blood type (A, B, AB, O; note: this is nominal).</li> <li> <strong>Note</strong>: For categorical variables, the chart above assumes that the classes are <code class="language-plaintext highlighter-rouge">nominal</code> (i.e. ordering of the classes does not matter). But, if the classes are <code class="language-plaintext highlighter-rouge">ordinal</code> (i.e. ordering <em>does</em> matter), then we use the same tests in the non-parametric part of continuous variables.</li> </ul> </li> <li> <strong>Survival variables</strong>: Think an hourglass. A survival variable is a unique type of data that has two parts: (i) a measure of time until a specific event occurs, and (ii) an indicator of whether the event has occurred or if the observation period ended before the event could happen (this is called censoring). A few examples: <ul> <li>Time until a patient relapses after treatment, with some patients still being disease-free at the end of the study (censored data).</li> <li>Time until a machine fails, with some machines still operational at the end of the observation period (censored data).</li> </ul> </li> </ul> <h4 id="exposure-independent-variable-types">Exposure (independent) variable types</h4> <ul> <li> <p><strong>Groups</strong>: They refer to the different methods/models we want to compare. For example, if we have three models A, B, and C, then we have three “groups”.</p> </li> <li> <p><strong>Samples</strong>: They refer to the number of observations we have for each group. For example, (i) each set of predictions from models A, B, and C is a sample (so three models, three sets of samples), (ii) if we have five different runs of model A, then we have five samples for model A, and so on.</p> </li> <li> <p><strong>Paired samples</strong>: They refer to the samples that are related to each other. For example, (i) if we have three models A, B, and C and we evaluate them on the same test set, then the samples from these models are “paired” because they are related to each other through the same test set, (ii) if we have a model that we run five times with different random seeds and evaluate them on the same test set, then the samples from these five runs are paired samples.</p> </li> <li> <p><strong>Independent samples</strong>: They refer to the samples that are <em>not</em> related to each other. For example, say we have a large test dataset and we create non-overlapping sub-test sets from it. Then, evaluating models A, B, and C on these different sub-test sets will give us independent samples.</p> </li> <li> <p><strong>Parametric</strong>: This class of tests assume that the data follows some “known” distribution (most commonly the normal distribution) and are based on the parameters like the mean and standard deviation. Normality of the data can be tested using D’Agostino and Pearson’s test, Shapiro-Wilk test, etc. If the p-value from these tests is greater than a significance level (typically 0.05), meaning that the null hypothesis ($H_0$; stating that the data is normally distributed) is <em>not</em> rejected. This implies that we don’t have enough evidence to conclude that the data are not normally distributed, but know that the data lack a significant deviation from the normal distribution (hence enabling the use of parametric tests).</p> </li> <li> <p><strong>Non-parametric</strong>: This class of tests do not assume that the data follow a normal distribution. They are typically used when the data is not normally distributed or when the sample size is small (typically less than 30 per group).</p> </li> </ul> <p><a href="https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/" rel="external nofollow noopener" target="_blank">Statistics How To</a> is more extensive in terms of the definitions covered. Do check it out!</p> <h4 id="errors-in-hypothesis-testing">Errors in hypothesis testing</h4> <p>When performing statistical tests, we typically start with a null hypothesis ($H_0$) and an alternative hypothesis ($H_a$). The null hypothesis usually states that there is no effect or no difference between the groups, while the alternative hypothesis states that there is an effect or a difference. Based on the results of the statistical test, we either reject the null hypothesis or fail to reject it. However, there are two types of errors that can occur in this process:</p> <ul> <li> <p><strong>Type I error (False Positive)</strong>: It occurs when we reject the null hypothesis ($H_0$) when it is actually true. In simpler terms, it’s like a false alarm where we think there is an effect or difference when there isn’t one (e.g. concluding that a drug is effective when it’s not). The significance level (alpha, typically set at 0.05) represents the probability of making a Type I error. For example, if we set alpha to 0.05, it means that we are willing to accept a 5% chance of incorrectly rejecting the null hypothesis.</p> </li> <li> <p><strong>Type II error (False Negative)</strong>: It occurs when we fail to reject the null hypothesis ($H_0$) when it is actually false. In simpler terms, it’s like missing a real effect or difference that actually exists (e.g. failing to detect that a drug is working). The probability of making a Type II error is denoted by beta (β). The power of a test, which is calculated as (1 - β), represents the probability of correctly rejecting the null hypothesis when it is false. A higher power means a lower chance of making a Type II error. Typically, researchers aim for a power of 0.8 or higher, meaning there is an 80% chance of correctly detecting an effect if it exists.</p> </li> </ul> <h3 id="choosing-the-right-test">Choosing the right test</h3> <p>When it comes to presenting the results, a recurring scenario is the following:</p> <blockquote> <p>We have the method/model that we have proposed and we want to compare its performance with the rest of the methods/models from the literature. We also have a fixed test set on which we evaluate our models on.</p> </blockquote> <p>Now, let’s take a bottom-up approach to arrive at the right test(s) to perform. Note that the test set is fixed so we have already reduced the search space for the class of the tests to be within: “2 groups” or “&gt;2 groups” with “paired” samples (since the models are evaluated on the same test set). If we have two models (model A vs. model B) then we use either paired Student’s t-test or Wilcoxon signed rank test if the variable we want to compare is continuous (e.g. accuracy, F1-score, etc.) and McNemar’s test if the variable is categorical (e.g. model predictions as <code class="language-plaintext highlighter-rouge">Spam</code>/<code class="language-plaintext highlighter-rouge">Not spam</code>). If we’re comparing more than two models (i.e. ours and two or more) then we use either repeated measures ANOVA or Friedman test if the variable we want to compare is continuous and Cochran’s Q test if the variable is categorical. The next natural question is: should we use parametric or non-parametric tests?</p> <h4 id="parametric-or-non-parametric">Parametric or Non-parametric?</h4> <p>When comparing continuous variables, the choice between choosing parametric or non-parametric tests depends on the distribution of the data. To ground it in ML terms, the “data” here refer to the set of performance metrics on the test set obtained from the models we want to compare. To know how the data are distributed we typically run normality tests from the <code class="language-plaintext highlighter-rouge">scipy.stats</code> package. Here’s a short code example of how to do it:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="c1"># Example data: performance metrics from two models
</span><span class="n">model_a_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">])</span>
<span class="n">model_b_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.81</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">])</span>
<span class="c1"># Perform D'Agostino and Pearson's test for normality
</span><span class="n">stat_a</span><span class="p">,</span> <span class="n">p_value_a</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">normaltest</span><span class="p">(</span><span class="n">model_a_metrics</span><span class="p">)</span>
<span class="n">stat_b</span><span class="p">,</span> <span class="n">p_value_b</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">normaltest</span><span class="p">(</span><span class="n">model_b_metrics</span><span class="p">)</span>
<span class="c1"># Significance level
# NOTE: The null hypothesis (H0) is that the data is normally distributed.
</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="c1"># Check if the data is normally distributed
</span><span class="n">is_a_normal</span> <span class="o">=</span> <span class="n">p_value_a</span> <span class="o">&gt;</span> <span class="n">alpha</span>
<span class="n">is_b_normal</span> <span class="o">=</span> <span class="n">p_value_b</span> <span class="o">&gt;</span> <span class="n">alpha</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model A normality: </span><span class="si">{</span><span class="n">is_a_normal</span><span class="si">}</span><span class="s">, p-value: </span><span class="si">{</span><span class="n">p_value_a</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model B normality: </span><span class="si">{</span><span class="n">is_b_normal</span><span class="si">}</span><span class="s">, p-value: </span><span class="si">{</span><span class="n">p_value_b</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># CHECK
# If p &gt; alpha, we fail to reject the null hypothesis --&gt; data is normally distributed --&gt; parametric
# If p &lt;= alpha, we reject the null hypothesis --&gt; data is not normally distributed --&gt; non-parametric
</span></code></pre></div></div> <p>Based on the normality tests, we now know whether to choose from parametric or non-parametric tests. More detailed definitions with examples can be found <a href="https://www.mayo.edu/research/documents/parametric-and-nonparametric-demystifying-the-terms/doc-20408960" rel="external nofollow noopener" target="_blank">here</a>. Continuing the example from the scenario mentioned above, say there are more than 2 groups (i.e. we’re comparing more than 2 methods) and the test indicates that there is a significant difference between the groups. How do we know which groups are different from each other? This is where post-hoc tests come into play.</p> <h4 id="post-hoc-tests">Post-hoc tests</h4> <p><em>Post-hoc</em> is a Latin term that means “after this”. <a href="https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/" rel="external nofollow noopener" target="_blank">Post-hoc tests</a> are additional tests performed after an initial statistical test (like ANOVA or Friedman test) to determine which specific groups are different from each other. They help to identify where the differences lie when the initial test indicates that there is a significant difference among the groups. More concretely, say we have three models A, B, and C and we perform a Friedman test (non-parametric) to compare their performance on a fixed test set. The test gives a p-value of 0.02 (which is significant!). But, we don’t know which pairs of models are significantly different from each other (i.e. A vs. B, A vs. C, B vs. C). Therefore, we perform post-hoc tests, which involve multiple pairwise comparisons between the groups to identify which specific pairs of groups have significant differences in their means or distributions. Some common post-hoc tests include:</p> <ul> <li> <strong>Tukey’s HSD (Honestly Significant Difference) test</strong>: used after ANOVA to find means that are significantly different from each other.</li> <li> <strong>Bonferroni correction</strong>: adjusts the significance level when multiple pairwise tests are performed to control the overall Type I error rate. This correction limits the possibility of getting a statistically significant result because, the more tests we run, the more likely we are to get a significant result. The correction essentially lowers the area where we can reject the null hypothesis by adjusting the alpha according to the number of tests. For e.g. after Bonferroni correction, \(\alpha_{\textrm{new}} = \alpha_{\textrm{old}} / N\), where $N$ is the number of tests or number of comparisons.</li> <li> <strong>Dunn’s test</strong>: non-parametric post-hoc test used after the Kruskal-Wallis test or Friedman test.</li> </ul> <p>Here’s a short code example on performing Dunn’s post-hoc test:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">scikit_posthocs</span> <span class="k">as</span> <span class="n">sp</span>
<span class="c1"># Example data: performance metrics from three models
</span><span class="n">model_a_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">])</span>
<span class="n">model_b_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.81</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">])</span>
<span class="n">model_c_metrics</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.78</span><span class="p">,</span> <span class="mf">0.77</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.76</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">])</span>
<span class="c1"># Combine the data into a single array
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">model_a_metrics</span><span class="p">,</span> <span class="n">model_b_metrics</span><span class="p">,</span> <span class="n">model_c_metrics</span><span class="p">])</span>
<span class="c1"># Create a group labels array
</span><span class="n">groups</span> <span class="o">=</span> <span class="p">([</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">model_a_metrics</span><span class="p">)</span> <span class="o">+</span>
          <span class="p">[</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">model_b_metrics</span><span class="p">)</span> <span class="o">+</span>
          <span class="p">[</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">model_c_metrics</span><span class="p">))</span>
<span class="c1"># Create a DataFrame for the data
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">Model</span><span class="sh">'</span><span class="p">:</span> <span class="n">groups</span><span class="p">,</span> <span class="sh">'</span><span class="s">Accuracy</span><span class="sh">'</span><span class="p">:</span> <span class="n">data</span><span class="p">})</span>
<span class="c1"># Perform Dunn's test with Holm-Bonferroni correction
</span><span class="n">dunn_results</span> <span class="o">=</span> <span class="n">sp</span><span class="p">.</span><span class="nf">posthoc_dunn</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">val_col</span><span class="o">=</span><span class="sh">'</span><span class="s">Accuracy</span><span class="sh">'</span><span class="p">,</span> <span class="n">group_col</span><span class="o">=</span><span class="sh">'</span><span class="s">Model</span><span class="sh">'</span><span class="p">,</span> <span class="n">p_adjust</span><span class="o">=</span><span class="sh">'</span><span class="s">holm</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dunn_results</span><span class="p">)</span>
</code></pre></div></div> <p>Note that post-hoc tests are only performed when the initial test indicates that there is a significant difference between the groups.</p> <p>Okay, we now have an understanding of how to go about choosing the right test. Let’s look at a few examples now.</p> <h3 id="examples">Examples</h3> <h4 id="example-1-comparing-accuracy-between-five-models-on-a-fixed-test-set">Example 1: Comparing accuracy between five models on a fixed test set</h4> <p>Say you have five models (A, B, C, D, E) and you want to compare their accuracy on a fixed test set. The workflow would be as follows:</p> <ol> <li>Since we’re comparing accuracy, the outcome variable is continuous.</li> <li>We have five models, so we have more than two groups.</li> <li>The samples are paired since the models are evaluated on the same test set.</li> <li>Check for normality on each of the test set predictions for the five models. If all the models’ predictions are normally distributed, we can use parametric tests. If at least one of them is not normally distributed, we use non-parametric tests.</li> <li>Choose the appropriate test. So, we have: <ul> <li> <code class="language-plaintext highlighter-rouge">continuous</code>, <code class="language-plaintext highlighter-rouge">&gt;2 groups</code>, <code class="language-plaintext highlighter-rouge">paired</code>, <code class="language-plaintext highlighter-rouge">non-parametric</code>: Friedman test</li> <li> <code class="language-plaintext highlighter-rouge">continuous</code>, <code class="language-plaintext highlighter-rouge">&gt;2 groups</code>, <code class="language-plaintext highlighter-rouge">paired</code>, <code class="language-plaintext highlighter-rouge">parametric</code>: Repeated measures ANOVA</li> </ul> </li> <li>If the test indicates that there is a significant difference between the groups, perform post-hoc test. For <code class="language-plaintext highlighter-rouge">parametric</code>, use Tukey’s HSD test. For <code class="language-plaintext highlighter-rouge">non-parametric</code>, use Dunn’s test.</li> </ol> <h4 id="example-2-comparing-ages-of-patients-in-train-and-test-sets">Example 2: Comparing ages of patients in train and test sets</h4> <p>Say you are working with medical data and you have randomly split the data into train/test but you want to know if the age of the patients in train set is significantly different from that in the test set (to make a case for your model’s generalization across age groups). The workflow would be as follows:</p> <ol> <li>Identify the type of outcome variable: <code class="language-plaintext highlighter-rouge">Age</code> is a continuous variable.</li> <li>Identify the number of groups: We have two groups (train and test).</li> <li>Identify the type of samples: The samples are independent since the train and test sets are disjoint.</li> <li>Check for normality: Use D’Agostino and Pearson’s test or Shapiro-Wilk test to check if the age data in both groups is normally distributed.</li> <li>Choose the appropriate test: <ul> <li> <code class="language-plaintext highlighter-rouge">continuous</code>, <code class="language-plaintext highlighter-rouge">2 groups</code>, <code class="language-plaintext highlighter-rouge">independent</code>, <code class="language-plaintext highlighter-rouge">non-parametric</code>: Mann-Whitney U test</li> <li> <code class="language-plaintext highlighter-rouge">continuous</code>, <code class="language-plaintext highlighter-rouge">2 groups</code>, <code class="language-plaintext highlighter-rouge">independent</code>, <code class="language-plaintext highlighter-rouge">parametric</code>: Student’s t-test (two-sample t-test)</li> </ul> </li> </ol> <h4 id="example-3-comparing-model-predictions-categorical-between-three-models-on-a-fixed-test-set">Example 3: Comparing model predictions (categorical) between three models on a fixed test set</h4> <p>Say you have three models (A, B, C) and you want to compare their predictions (as <code class="language-plaintext highlighter-rouge">Malignant</code>/<code class="language-plaintext highlighter-rouge">Benign</code>) on a fixed test set. The workflow would be as follows:</p> <ol> <li>Since we’re comparing models’ binary classification predictions, the outcome variable is categorical.</li> <li>We have three models, so we have more than two groups.</li> <li>The samples are paired since the models are evaluated on the same test set.</li> <li>Choose the appropriate test. So, we have: <ul> <li> <code class="language-plaintext highlighter-rouge">categorical</code>, <code class="language-plaintext highlighter-rouge">&gt;2 groups</code>, <code class="language-plaintext highlighter-rouge">paired</code>: Cochran’s Q test</li> </ul> </li> </ol> <h3 id="general-notes-and-common-pitfalls">General Notes and Common Pitfalls</h3> <h4 id="choosing-significance-level-alpha">Choosing significance level (alpha)</h4> <p>A good significance level balances the risks of false positives and false negatives. Choosing the right significance level depends on the context and the consequence of the errors. A significance level of 5% is more commonly used than that of 1% which is more conservative and used only in safety-critical applications like medicine.</p> <h4 id="ignoring-the-multiple-comparisons-problem">Ignoring the multiple comparisons problem</h4> <p>When we have 3 or more groups to compare, running multiple pairwise tests (i.e. A vs. B, A vs. C, B vs. C) should <em>not</em> be done as multiple comparisons increase the chance of getting a “significant” result purely by luck. As each test has its own significance level (alpha), the overall chance of getting a false positive increases with multiple comparisons. Therefore, in this scenario, it is recommended to run a Friedman test or repeated measures ANOVA, and if it is significant, only then run post hoc tests to find specific differences between groups.</p> <h4 id="when-to-not-rely-purely-on-p-values">When to <em>not</em> rely purely on p-values</h4> <p>It is critical to note that the p-value is influenced by the sample size. Larger sample sizes increase statistical power, making it easier to detect even small, non-random effects, leading to a statistically significant result. Large sample sizes increase the ability to detect true, small effects that might otherwise be hidden by random noise. And with a large sample, the p-value becomes smaller for the same observed effect, thereby increasing the likelihood of rejecting the null hypothesis (and getting a significant result). In such cases, reporting the “Effect size” along with statistical significance can help put the results into perspective.</p> <ul> <li> <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3444174/" rel="external nofollow noopener" target="_blank">Effect Size</a>: While p-values (statistical significance) answers the question <em>“is there an effect?”</em>, effect size answers <em>“how meaningful/practical is the effect?”</em>. The <em>effect</em> could be difference between groups, strength of a relationship between two continous variables. <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient]" rel="external nofollow noopener" target="_blank">Pearson’s <code class="language-plaintext highlighter-rouge">r</code></a> and <a href="https://statisticsbyjim.com/basics/cohens-d/" rel="external nofollow noopener" target="_blank">Cohen’s <code class="language-plaintext highlighter-rouge">d</code></a> are two common examples used to compute effect sizes.</li> </ul> <p>I only focused on examples of contiguous and categorical variables in this post because those are what I came across most often. I will update this post with examples of survival variables and their tests in the future.</p> <p>Thanks to Kalum Ost, Jan Valošek, Pierre-Louis Benveniste, and Gemini 2.5 Pro for their suggestions on the drafts of this post.</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Naga Karthik Enamundram. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>